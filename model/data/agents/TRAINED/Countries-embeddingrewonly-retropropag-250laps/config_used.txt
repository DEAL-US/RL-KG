guided_reward: True
guided_to_compute: ['terminal', 'embedding']
use_LSTM: True
use_episodes: False
episodes: 0
path_length: 10
alpha: 0.9
gamma: 0.99
learning_rate: 0.0001
activation: leaky_relu
regularizers: []
algorithm: BASE
reward_type: simple
reward_computation: one_hot_max
action_picking_policy: probability
laps: 250
dataset: COUNTRIES
embedding_index: 0
single_relation_pair: [False, None]
